{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4897362",
   "metadata": {},
   "source": [
    "# ISM Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337cf975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from decimal import Decimal, getcontext\n",
    "import Lya_zelda as Lya\n",
    "import itertools\n",
    "import h5py\n",
    "\n",
    "from astropy import units as u\n",
    "from astropy.cosmology import FlatLambdaCDM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6508a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cosmology and units using astropy\n",
    "cosmo = FlatLambdaCDM(H0=67.77, Om0=0.307115)\n",
    "littleh = cosmo.H0.value / 100.0\n",
    "Mpcph = u.def_unit('Mpcph', u.Mpc / littleh)  # [Mpc/h]\n",
    "Msunph = u.def_unit('Msunph', u.Msun / littleh)  # [Msun/h]\n",
    "kpcph = u.def_unit('kpcph', u.kpc / littleh)  # [kpc/h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c485b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data including scaling relation and luminosiy cut.\n",
    "\n",
    "dat = np.load(\"sfr_catalog_0.258000_w_scaling_relations_masked_L41_pos400.npy\", allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72e8706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_v_exp(alpha, data, h5_filename):\n",
    "    \"\"\"\n",
    "    Calculates the v_exp using the parametrization: v_exp = alpha * SFR * (r/sm)\n",
    "    and saves into a .h5 file\n",
    "    \n",
    "    Inputs:\n",
    "    -------\n",
    "    alpha --> free parameter\n",
    "    data --> load the catalog to get sfr, sm and galaxy size.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    v_exp --> expansion velocity of the thin shell. (array: (shape=(N,)) --> [unit: km/s]\n",
    "    \n",
    "    \"\"\"\n",
    "    sm = data[\"sm\"]\n",
    "    sfr = data[\"sfr\"]\n",
    "    size = data[\"size\"]\n",
    "\n",
    "    myr = 10 ** size\n",
    "    sm_h = ((sm * u.Msun).to(Msunph)).value\n",
    "    sfr_h_s = ((sfr * u.Msun / u.yr).to(Msunph / u.s)).value\n",
    "    myr_km = ((myr * u.kpc).to(u.km)).value\n",
    "    v_exp = alpha * sfr_h_s * (myr_km / sm_h)\n",
    "\n",
    "    # Save the result in the HDF5 file with alpha and beta information\n",
    "    with h5py.File(h5_filename, 'w') as file:\n",
    "        # Create a group with the current alpha value\n",
    "        group_name = f'alpha_{alpha}'\n",
    "        group = file.create_group(group_name)\n",
    "\n",
    "        # Save the v_exp dataset within the group\n",
    "        dset_v_exp = group.create_dataset('v_exp_Arr', data=v_exp, compression=\"gzip\", compression_opts=9, chunks=True, maxshape=(None,))\n",
    "\n",
    "        # Add attributes to the group to store alpha value\n",
    "        group.attrs['alpha'] = alpha\n",
    "\n",
    "    #print('vexp is done!')\n",
    "    return v_exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a74cef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_N_HI(alpha, beta, data, h5_filename):\n",
    "    \"\"\"\n",
    "    Calculates the N_HI using the parametrization: N_HI = beta * (mcold/4*pi*m_H*r**2)\n",
    "    and saves into a .h5 file\n",
    "    \n",
    "    Inputs:\n",
    "    -------\n",
    "    alpha --> free parameter\n",
    "    beta --> free parameter\n",
    "    data --> load the catalog to get mcold and zcold.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    N_HI --> column density of hydrogen of the thin shell. (array: (shape=(N,)) --> [unit: 1/cm2]\n",
    "    \n",
    "    \"\"\"\n",
    "    mcold = data[\"mcold\"]\n",
    "    zcold = data[\"zcold\"]\n",
    "    sample_M_cold_not_log = 10 ** mcold\n",
    "    # Vectorize the Decimal conversion function\n",
    "    decimal_converter = np.vectorize(lambda x: Decimal(str(x)))\n",
    "    sample_M_cold_not_log_decimal = decimal_converter(sample_M_cold_not_log)\n",
    "    result_decimal = sample_M_cold_not_log_decimal * Decimal(\"1.9891e30\")\n",
    "    mcold_nolog = np.array(result_decimal, dtype=float)\n",
    "    M_c = (mcold_nolog / 0.70)\n",
    "    myr = 10 ** data[\"size\"]\n",
    "    myr_km = ((myr * u.kpc).to(u.km)).value\n",
    "    myr_cm = ((myr_km * u.km).to(u.cm)).value\n",
    "    # Use NumPy vectorized operations for the remaining calculations\n",
    "    getcontext().prec = 50\n",
    "    rrr_decimal = np.array(myr_cm, dtype=object)  # Using dtype=object to handle Decimal\n",
    "    rrr_decimal = np.vectorize(Decimal)(rrr_decimal)\n",
    "    result_decimal_rrr = 1 / rrr_decimal ** 2\n",
    "    myr_cm_new = np.array(result_decimal_rrr, dtype=float)\n",
    "    N_HI_new = beta * (M_c * myr_cm_new)\n",
    "\n",
    "    # Save the result in the HDF5 file with alpha and beta information\n",
    "    with h5py.File(h5_filename, 'w') as file:\n",
    "        # Create a group with the current alpha and beta values\n",
    "        group_name = f'alpha_{alpha}_beta_{beta}'\n",
    "        group = file.create_group(group_name)\n",
    "\n",
    "        # Save the N_HI dataset within the group\n",
    "        dset_N_HI = group.create_dataset('N_HI_Arr', data=N_HI_new, compression=\"gzip\", compression_opts=9, chunks=True, maxshape=(None,))\n",
    "\n",
    "        # Add attributes to the group to store alpha and beta values\n",
    "        group.attrs['alpha'] = alpha\n",
    "        group.attrs['beta'] = beta\n",
    "\n",
    "    #print('NHI is done!')\n",
    "    return N_HI_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8d00c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tau(alpha, beta, data, h5_filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the tau (dust optical depth) using the parametrization:  (1 - A_Lya) * (E_Sun / Z_Sun) * N_HI * Z_c\n",
    "    and saves into a .h5 file\n",
    "    \n",
    "    Inputs:\n",
    "    -------\n",
    "    alpha --> free parameter\n",
    "    beta --> free parameter\n",
    "    data --> load the catalog to get mcold and zcold.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tau_a --> dust optical depth. (array: (shape=(N,))\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    N_HI_new = calculate_N_HI(alpha, beta, data, h5_filename)  # Pass the h5_filename to calculate_N_HI\n",
    "\n",
    "    A_Lya = 0.39\n",
    "    E_Sun = 1.77e-21\n",
    "    Z_Sun = 0.02\n",
    "    Z_c = 10 ** data[\"zcold\"]\n",
    "    tau_new = (1 - A_Lya) * (E_Sun / Z_Sun) * N_HI_new * Z_c\n",
    "\n",
    "    # Save the result in the HDF5 file with alpha and beta information\n",
    "    with h5py.File(h5_filename, 'a') as file:  # Open the file in 'a' (append) mode\n",
    "        # Get the existing group with the current alpha and beta values\n",
    "        group_name = f'alpha_{alpha}_beta_{beta}'\n",
    "        group = file[group_name]\n",
    "\n",
    "        # Save the tau dataset within the group\n",
    "        dset_tau = group.create_dataset('tau_Arr', data=tau_new, compression=\"gzip\", compression_opts=9, chunks=True, maxshape=(None,))\n",
    "\n",
    "    #print('tau is done!')\n",
    "    return tau_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d862c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f_esc(alpha, beta, data, h5_filename):\n",
    "    \"\"\"\n",
    "    Calculates the tau (Dust optical depth) using the parametrization:  (1 - A_Lya) * (E_Sun / Z_Sun) * N_HI * Z_c\n",
    "    and saves into a .h5 file\n",
    "    \n",
    "    Here we use zELDA code (written by Siddhartha Gurung-Lopez). In your environment do the following before using this function: \n",
    "    pip install zelda \n",
    "    and download the Grids as follows:\n",
    "    curl --cookie zenodo-cookies.txt \"https://zenodo.org/record/4733518/files/Grids.zip?download=1\" --output Grids.zip\n",
    "    \n",
    "    Inputs:\n",
    "    -------\n",
    "    alpha --> free parameter\n",
    "    beta --> free parameter\n",
    "    data --> load the catalog to get mcold and zcold.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tau_a --> dust optical depth. (array: (shape=(N,))\n",
    "    \n",
    "    \"\"\"\n",
    "    vexp = calculate_v_exp(alpha, data, h5_filename)\n",
    "    N_HI = calculate_N_HI(alpha, beta, data, h5_filename)  # Pass the h5_filename to calculate_N_HI\n",
    "    tau = calculate_tau(alpha, beta, data, h5_filename)  # Pass the h5_filename to calculate_tau\n",
    "    log_N_HI = np.log10(N_HI)\n",
    "    log_tau = np.log10(tau)\n",
    "    \n",
    "    # Configuration for Zelda\n",
    "    my_grids_location = '/Volumes/Hasti-2T-2/main/zelda/Grids/'  # change this part depending to where you download the Grid from zELDA\n",
    "    Lya.funcs.Data_location = my_grids_location\n",
    "\n",
    "    # Specify the geometry and prepare input arrays\n",
    "    Geometry = 'Thin_Shell'  # Other options: 'Galactic Wind' or 'Bicone_X_Slab_In' or 'Bicone_X_Slab_Out'\n",
    "    V_Arr = np.array(vexp)  # Expansion velocity array in km/s\n",
    "    logNH_Arr = np.array(log_N_HI)  # Logarithmic of column densities array in cm**-2\n",
    "    tau_Arr = np.array(log_tau)  # Dust optical depth Array\n",
    "\n",
    "    # Run Zelda and calculate f_esc\n",
    "    f_esc_Arr = Lya.RT_f_esc(Geometry, V_Arr, logNH_Arr, tau_Arr)\n",
    "\n",
    "    # Save the result in the HDF5 file with alpha and beta information\n",
    "    with h5py.File(h5_filename, 'a') as file:  # Open the file in 'a' (append) mode\n",
    "        # Get the existing group with the current alpha and beta values\n",
    "        group_name = f'alpha_{alpha}_beta_{beta}'\n",
    "        group = file[group_name]\n",
    "\n",
    "        # Save the f_esc dataset within the group\n",
    "        dset_f_esc = group.create_dataset('f_esc_Arr', data=f_esc_Arr, compression=\"gzip\", compression_opts=9, chunks=True, maxshape=(None,))\n",
    "\n",
    "    #print('f_esc is done!')\n",
    "    return f_esc_Arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea1b1fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for alpha = 105, beta = 9.530236113287146e+21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/Lya_zelda/funcs.py:284: RuntimeWarning: invalid value encountered in power\n",
      "  f_esc = 1./np.cosh( np.sqrt( CCC * (ta**KKK) ) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All calculations are done.\n",
      "CPU times: user 1min 6s, sys: 826 ms, total: 1min 7s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define the range of values for alpha and beta\n",
    "# Here I just run it for one example of alpha and beta:\n",
    "alpha_values = np.array([105])\n",
    "betaa_values = np.array([2e-4])\n",
    "\n",
    "# Define the range of values for alpha and beta\n",
    "#alpha_values = np.array([5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100,\n",
    "#                        105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180,\n",
    "#                        185, 190, 195, 200])\n",
    "\n",
    "#betaa_values = np.array([0.6e-4, 0.7e-4, 0.8e-4, 0.9e-4, 1e-4,\n",
    "#                        1.1e-4, 1.2e-4, 1.3e-4, 1.4e-4, 1.5e-4, 1.6e-4, 1.7e-4, 1.8e-4, 1.9e-4, 2e-4,\n",
    "#                        2.1e-4, 2.2e-4, 2.3e-4, 2.4e-4, 2.5e-4, 2.6e-4, 2.7e-4, 2.8e-4, 2.9e-4, 3e-4,\n",
    "#                        3.1e-4, 3.2e-4, 3.3e-4, 3.4e-4, 3.5e-4, 3.6e-4, 3.7e-4, 3.8e-4, 3.9e-4, 4e-4,\n",
    "#                        4.1e-4, 4.2e-4, 4.3e-4, 4.4e-4, 4.5e-4, 4.6e-4, 4.7e-4, 4.8e-4, 4.9e-4, 5e-4])\n",
    "\n",
    "M_Sun = 1.9891e30\n",
    "m_H = 1.67e-27  # in kg\n",
    "beta_values = betaa_values / (4 * np.pi * m_H)\n",
    "\n",
    "# Use itertools.product to generate all combinations\n",
    "combinations = list(itertools.product(alpha_values, beta_values))\n",
    "\n",
    "# Loop over all combinations\n",
    "for alpha, beta in combinations:\n",
    "    print(f\"Calculating for alpha = {alpha}, beta = {beta}\")\n",
    "\n",
    "    # Create an HDF5 file for the current combination with compression and chunking\n",
    "    h5_filename = f'f_esc_ISM_alpha_{alpha}_beta_{beta}_a=0.258000_z=2.88.h5'\n",
    "\n",
    "    # Call the function with the current combination\n",
    "    f_esc_Arr = calculate_f_esc(alpha, beta, dat, h5_filename)\n",
    "\n",
    "    # Save the result in the HDF5 file\n",
    "    with h5py.File(h5_filename, 'w') as file:\n",
    "        dset = file.create_dataset('f_esc_Arr', data=f_esc_Arr, compression=\"gzip\", compression_opts=9, chunks=True, maxshape=(None,))\n",
    "\n",
    "print(\"All calculations are done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee42400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
